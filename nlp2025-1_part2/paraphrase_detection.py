# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import gc                                 # Garbage Collection ì œì–´ìš©
import argparse                           # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬ìš©
import random                             # íŒŒì´ì¬ random seed ì„¤ì •ìš©
import torch                              # PyTorch ë©”ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np                        # ë„˜íŒŒì´, ìˆ˜ì¹˜ ì—°ì‚°ìš©
import torch.nn.functional as F           # ì†ì‹¤í•¨ìˆ˜ ë“± ì‹ ê²½ë§ ì—°ì‚° í•¨ìˆ˜
from torch import nn                      # ì‹ ê²½ë§ êµ¬ì„± ìš”ì†Œ
from torch.utils.data import DataLoader   # ë°°ì¹˜ ë‹¨ìœ„ ë°ì´í„° ë¡œë”
from tqdm import tqdm                     # ì§„í–‰ë¥  ì‹œê°í™”

# ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ë° ë¡œë” ê´€ë ¨ í•¨ìˆ˜
from datasets import (
  ParaphraseDetectionDataset,         # í•™ìŠµ/ê²€ì¦ìš© ì»¤ìŠ¤í…€ Dataset í´ë˜ìŠ¤
  ParaphraseDetectionTestDataset,     # í…ŒìŠ¤íŠ¸ìš© Dataset í´ë˜ìŠ¤
  load_paraphrase_data                # CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
)

# ëª¨ë¸ í‰ê°€ ë° í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
from evaluation import model_eval_paraphrase, model_test_paraphrase
from optimizer import AdamW                               # ì‚¬ìš©ì ì •ì˜ AdamW ì˜µí‹°ë§ˆì´ì € (torch.optim.AdamWì™€ ë™ì¼í•˜ê±°ë‚˜ ì»¤ìŠ¤í…€)
from sklearn.metrics import accuracy_score, f1_score      # ì •í™•ë„, F1-score ë“± ì„±ëŠ¥ ì¸¡ì •ìš©
from transformers import GPT2Model                        # Huggingface GPT-2 ëª¨ë¸
from peft import get_peft_model, LoraConfig, TaskType     # LoRA ì ìš©ì„ ìœ„í•œ PEFT ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch.nn.utils.rnn import pad_sequence               # ë°°ì¹˜ ë‹¨ìœ„ íŒ¨ë”©ì„ ìœ„í•œ í•¨ìˆ˜

TQDM_DISABLE = False    # tqdm ë¹„í™œì„±í™” ì—¬ë¶€



# ëœë¤ ì‹œë“œ ê³ ì • í•¨ìˆ˜ (ì¬í˜„ì„± í™•ë³´ ëª©ì )
def seed_everything(seed=11711):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  torch.cuda.manual_seed_all(seed)
  torch.backends.cudnn.benchmark = False
  torch.backends.cudnn.deterministic = True



# LoRAê°€ ì ìš©ëœ GPT2 ë˜í¼ í´ë˜ìŠ¤ ì •ì˜
class LoraGPT2Wrapper(nn.Module):
    
    def __init__(self, model_name='gpt2'):
        super().__init__()
        self.model_name = model_name

        # HuggingFaceì—ì„œ ì‚¬ì „í•™ìŠµëœ GPT2 ëª¨ë¸ ë¡œë“œ
        self.base_model = GPT2Model.from_pretrained(model_name)

        # LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=8,
            lora_alpha=32,
            lora_dropout=0.1,
            bias="none",
            target_modules=["c_attn", "c_proj"]
        )

        # PEFT ëª¨ë“ˆë¡œ ëª¨ë¸ ë³€í™˜ (LoRA ì ìš©)
        self.peft_model = get_peft_model(self.base_model, peft_config)


    # LoRAê°€ ì ìš©ëœ GPT-2 ëª¨ë¸ forward ìˆ˜í–‰
    def forward(self, input_ids, attention_mask):
        return self.peft_model(input_ids=input_ids, attention_mask=attention_mask)



# Paraphrase Detection GPT-2 ëª¨ë¸ ì •ì˜
class ParaphraseGPT(nn.Module):
    
    def __init__(self, args):
        super().__init__()    
        self.gpt = LoraGPT2Wrapper(model_name=args.model_size)    # LoRAê°€ ì ìš©ëœ GPT2 ë˜í¼ í´ë˜ìŠ¤ ì´ˆê¸°í™”

        # GPT2 ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ hidden size ì§€ì •
        if args.model_size == 'gpt2':
            hidden_size = 768
        elif args.model_size == 'gpt2-medium':
            hidden_size = 1024
        elif args.model_size == 'gpt2-large':
            hidden_size = 1280
        else:
            raise ValueError(f"Unsupported model size: {args.model_size}")

        self.paraphrase_detection_head = nn.Linear(hidden_size, 2)    # ë§ˆì§€ë§‰ hidden stateë¥¼ ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë ˆì´ì–´ë¡œ íˆ¬ì‚¬


    def forward(self, input_ids, attention_mask):
        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)                  # LoRA ì ìš© GPT-2 forward ìˆ˜í–‰
        seq_lengths = attention_mask.sum(dim=1) - 1                                             # ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì‹¤ì œ í† í° ìœ„ì¹˜ ì¸ë±ìŠ¤ ê³„ì‚° (íŒ¨ë”© ì œì™¸)
        last_hidden = outputs.last_hidden_state[torch.arange(input_ids.size(0)), seq_lengths]   # ë§ˆì§€ë§‰ í† í°ì˜ hidden state ì¶”ì¶œ
        logits = self.paraphrase_detection_head(last_hidden)                                    # ì„ í˜• ë¶„ë¥˜ê¸° í†µê³¼í•˜ì—¬ logits ìƒì„± (2 í´ë˜ìŠ¤)
        return logits 



# Hard Negative ìƒ˜í”Œ ìˆ˜ì§‘ í•¨ìˆ˜
def collect_hard_negatives(dataloader, model, device, threshold=0.7):
  
  # ëª¨ë¸ì´ í‹€ë¦¬ê²Œ ì˜ˆì¸¡í–ˆìœ¼ë‚˜ í™•ì‹ (confidence)ì€ ë†’ì€ ìƒ˜í”Œ ìˆ˜ì§‘
  model.eval()
  hard_negatives = []
  
  with torch.no_grad(): 
    for batch in tqdm(dataloader, desc="Collecting Hard Negatives", disable=TQDM_DISABLE):
        b_ids = batch['token_ids'].to(device)
        b_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device).flatten()

        # ì˜ˆì¸¡ ë° softmax í™•ë¥  ê³„ì‚°
        logits = model(b_ids, b_mask)
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

        # ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì •ë‹µê³¼ ë‹¤ë¥´ë©´ì„œë„ í™•ì‹ (confidence)ì´ ë†’ì€ ê²½ìš°
        confidences = probs[torch.arange(len(preds)), preds]
        mask = (preds != labels) & (confidences > threshold)

        # í•´ë‹¹ ìƒ˜í”Œì„ hard negativeë¡œ ìˆ˜ì§‘
        for i in range(len(labels)):
            if mask[i]:
                hard_negatives.append((
                    b_ids[i].detach().cpu(),    # ì…ë ¥ ID
                    b_mask[i].detach().cpu(),   # attention mask
                    labels[i].detach().cpu()    # ì‹¤ì œ ì •ë‹µ ë¼ë²¨
                ))

  # ìˆ˜ì§‘ëœ hard negativeì˜ ë ˆì´ë¸” ë¶„í¬ ì¶œë ¥
  print(f" â–¶ HNP ë¼ë²¨ ë¶„í¬: label=0 â†’ {sum(x[2].item() == 0 for x in hard_negatives)}, label=1 â†’ {sum(x[2].item() == 1 for x in hard_negatives)}")
  
  return hard_negatives



# Hard Negative Fine-Tuning í•¨ìˆ˜
def fine_tune_on_hard_negatives(model, args, device, train_dataloader, dev_dataloader):
    
    # 1ë‹¨ê³„: Hard Negative ìˆ˜ì§‘
    print("\nğŸ” Collecting hard negatives from train set...")
    hard_negatives = collect_hard_negatives(train_dataloader, model, device)
    print(f" Collected {len(hard_negatives)} hard negatives.")

    # 2ë‹¨ê³„: Soft Positive ìƒ˜í”Œ ìˆ˜ì§‘ (ì •ë‹µ ë§ì·„ì§€ë§Œ confidence ë‚®ì€ ìƒ˜í”Œ ì¤‘ ì¼ë¶€ë§Œ ëœë¤ ì„ íƒ)
    print("ğŸ“¥ Sampling soft positives from original training data...")
    model.eval()
    soft_positives = []

    with torch.no_grad():
        for batch in tqdm(train_dataloader, desc="Collecting Soft Positives", disable=TQDM_DISABLE):
            b_ids = batch['token_ids'].to(device)
            b_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].flatten().to(device)

            logits = model(b_ids, b_mask)
            probs = F.softmax(logits, dim=1)
            preds = torch.argmax(probs, dim=1)

            confidences = probs[torch.arange(len(preds)), preds]
            correct_mask = (preds == labels) & (confidences < 0.8)  # ë‚®ì€ confidenceì˜ ì •ë‹µ

            for i in range(len(labels)):
                if correct_mask[i] and random.random() < args.soft_pos_ratio:   # soft_pos_ratio ê¸°ë°˜ ëœë¤ ìƒ˜í”Œë§
                    soft_positives.append((
                        b_ids[i].detach().cpu(),
                        b_mask[i].detach().cpu(),
                        labels[i].detach().cpu()
                    ))

    print(f" âœ… Added {len(soft_positives)} soft positives to hard negatives.")

    # HNP + soft positive í•©ì¹˜ê¸°
    combined_samples = hard_negatives + soft_positives

    # í´ë˜ìŠ¤ ë³„ ê°œìˆ˜ ì„¸ê¸° ë° ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë³´ì •)
    label_counts = [0, 0]
    for item in combined_samples:
        label_counts[item[2].item()] += 1

    total = sum(label_counts)
    class_weights = [total / c if c > 0 else 0 for c in label_counts]
    class_weights = torch.tensor(class_weights).to(device)

    print(f" â–¶ Combined Label Count: 0 â†’ {label_counts[0]}, 1 â†’ {label_counts[1]}")
    print(f" â–¶ Computed Class Weights: {class_weights}")

    # HNP í•™ìŠµìš© ì˜µí‹°ë§ˆì´ì € ì„¤ì • (weight_decayë¡œ ì •ê·œí™” íš¨ê³¼)
    hnp_optimizer = AdamW(model.parameters(), lr=1e-6, weight_decay=0.01)
    

    # í•˜ë“œ ë„¤ê±°í‹°ë¸Œë¥¼ ìœ„í•œ ë°°ì¹˜ êµ¬ì„± í•¨ìˆ˜ ì •ì˜
    def collate_batch(batch):
        token_ids = pad_sequence([x[0] for x in batch], batch_first=True)
        attention_masks = pad_sequence([x[1] for x in batch], batch_first=True)
        labels = torch.stack([x[2] for x in batch])
        return token_ids, attention_masks, labels
    
    # 7ë‹¨ê³„: ê²°í•©ëœ ë°ì´í„°ë¡œ DataLoader êµ¬ì„±
    combined_dataloader = DataLoader(combined_samples, shuffle=True, batch_size=args.batch_size,
                                     collate_fn=collate_batch)
    
    # HNP í•™ìŠµ (1 epoch)
    for epoch in range(1):
        model.train()
        train_loss = 0
        for batch in tqdm(combined_dataloader, desc='fine-tune', disable=TQDM_DISABLE):
            b_ids, b_mask, labels = [x.to(device) for x in batch]
            hnp_optimizer.zero_grad()
            logits = model(b_ids, b_mask)
            loss = F.cross_entropy(logits, labels, reduction='mean', weight=class_weights)
            loss.backward()
            hnp_optimizer.step()
            train_loss += loss.item()

        print(f"Fine-tune Epoch {epoch}: loss = {train_loss / len(combined_dataloader):.4f}")
        dev_acc, dev_f1, *_ = model_eval_paraphrase(dev_dataloader, model, device)
        print(f"Dev accuracy after fine-tune-{epoch}: {dev_acc:.4f}, f1-score: {dev_f1:.4f}")

    return hnp_optimizer



# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ (ê¸°ë³¸ í•™ìŠµ + HNP Fine-tuning í¬í•¨)
def train(args):
  
  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')

  # Quora ë°ì´í„° ë¡œë“œ (í•™ìŠµ ë° dev ë°ì´í„°)
  para_train_data = load_paraphrase_data(args.para_train)
  para_dev_data = load_paraphrase_data(args.para_dev)

  # ì»¤ìŠ¤í…€ Dataset í´ë˜ìŠ¤ ì ìš©
  para_train_data = ParaphraseDetectionDataset(para_train_data, args)
  para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)

  # DataLoader ìƒì„± (shuffleì€ trainë§Œ ì ìš©)
  para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args.batch_size,
                                     collate_fn=para_train_data.collate_fn)
  para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,
                                   collate_fn=para_dev_data.collate_fn)

  args = add_arguments(args)    # ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì¸ì ë³´ì™„

  # ëª¨ë¸ ì´ˆê¸°í™” ë° ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
  model = ParaphraseGPT(args)
  model = nn.DataParallel(model)
  model = model.to(device)
  print(device)

  # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
  optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.)
  best_dev_acc = 0    # dev ìµœê³  ì •í™•ë„ ê¸°ë¡ìš©

  # ê¸°ë³¸ í•™ìŠµ ë£¨í”„ (Epoch ë‹¨ìœ„)
  for epoch in range(args.epochs):
    model.train()
    train_loss = 0
    num_batches = 0

    for batch in tqdm(para_train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
      # ì…ë ¥ ë°ì´í„°ì™€ ë¼ë²¨ GPUë¡œ ì´ë™
        b_ids = batch['token_ids'].to(device)
        b_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].flatten().to(device)

      # ìˆœì „íŒŒ, ì†ì‹¤ ê³„ì‚°, ì—­ì „íŒŒ, íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        optimizer.zero_grad()
        logits = model(b_ids, b_mask)
        loss = F.cross_entropy(logits, labels, reduction='mean')
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        num_batches += 1

    train_loss = train_loss / num_batches   # í‰ê·  í•™ìŠµ ì†ì‹¤ ê³„ì‚°
    dev_acc, dev_f1, *_ = model_eval_paraphrase(para_dev_dataloader, model, device)   # ê°œë°œ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€

    # ìµœê³  ì„±ëŠ¥ ê°±ì‹  ì‹œ ëª¨ë¸ ì €ì¥
    if dev_acc > best_dev_acc:
        best_dev_acc = dev_acc
        save_model(model, optimizer, args, args.filepath)

    print(f"Epoch {epoch}: train loss :: {train_loss :.3f}, dev acc :: {dev_acc :.3f}")
    gc.collect()    # ë©”ëª¨ë¦¬ ì •ë¦¬
    torch.cuda.empty_cache()

  gc.collect()    # ë©”ëª¨ë¦¬ ì •ë¦¬
  torch.cuda.empty_cache()

  # HNP í•™ìŠµ ì „: best ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ
  print("\nğŸ” Loading best model before HNP fine-tuning...")
  saved = torch.load(args.filepath, weights_only=False)
  model.load_state_dict(saved['model'], strict=False)
  model = model.to(device)

  # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œ ê¸°ë°˜ ì¶”ê°€ í•™ìŠµ ìˆ˜í–‰
  hnp_optimizer = fine_tune_on_hard_negatives(model, args, device, para_train_dataloader, para_dev_dataloader)

  # Fine-tune í›„ ëª¨ë¸ ì €ì¥
  finetuned_path = args.filepath.replace(".pt", "-hnp.pt")
  save_model(model, hnp_optimizer, args, finetuned_path)

  # HNP í•™ìŠµ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ì¶œë ¥
  dev_acc, dev_f1, *_ = model_eval_paraphrase(para_dev_dataloader, model, device)
  print(f"âœ… [After HNP Fine-Tune] Dev accuracy: {dev_acc:.4f}, f1-score: {dev_f1:.4f}")

  # âœ… 5 epoch ì¤‘ best ëª¨ë¸ ì„±ëŠ¥ ë‹¤ì‹œ ì¶œë ¥ (ìµœì¢… ì •ë¦¬)
  print("\nğŸ“Š Final Evaluation of Best 5epoch Model (before HNP)...")
  model.load_state_dict(saved['model'], strict=False)
  model = model.to(device)
  model.eval()

  dev_acc, dev_f1, *_ = model_eval_paraphrase(para_dev_dataloader, model, device)
  print(f" âœ… [Best of 5 Epochs] Dev accuracy: {dev_acc:.4f}, f1-score: {dev_f1:.4f}")



# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_model(model, optimizer, args, filepath):
  # ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, í•™ìŠµ ì„¤ì •, ë‚œìˆ˜ ì‹œë“œ ìƒíƒœê¹Œì§€ ì €ì¥
  save_info = {
    'model': model.state_dict(),
    'optim': optimizer.state_dict(),
    'args': args,
    'system_rng': random.getstate(),
    'numpy_rng': np.random.get_state(),
    'torch_rng': torch.random.get_rng_state(),
  }

  torch.save(save_info, filepath)
  print(f"save the model to {filepath}")



# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜: dev/test ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°ê³¼ íŒŒì¼ ì €ì¥
@torch.no_grad()
def test(args):

  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')

  # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
  saved = torch.load(args.filepath, weights_only=False)
  model = ParaphraseGPT(saved['args'])
  model = nn.DataParallel(model)
  model.load_state_dict(saved['model'], strict=False)
  model = model.to(device)
  model.eval()
  print(f"Loaded model to test from {args.filepath}")

  # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
  para_dev_data = load_paraphrase_data(args.para_dev)
  para_test_data = load_paraphrase_data(args.para_test, split='test')

  para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)
  para_test_data = ParaphraseDetectionTestDataset(para_test_data, args)

  para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,
                                   collate_fn=para_dev_data.collate_fn)
  para_test_dataloader = DataLoader(para_test_data, shuffle=False, batch_size=args.batch_size,
                                    collate_fn=para_test_data.collate_fn)

  # ê°œë°œì…‹ ë° í…ŒìŠ¤íŠ¸ì…‹ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
  dev_para_acc, dev_para_f1, dev_para_y_pred, dev_para_y_true, dev_para_sent_ids = model_eval_paraphrase(para_dev_dataloader, model, device)
  test_para_y_pred, test_para_sent_ids = model_test_paraphrase(para_test_dataloader, model, device)

  # ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ì €ì¥ (dev)
  with open(args.para_dev_out, "w+") as f:
    f.write(f"id \t Predicted_Is_Paraphrase \n")
    for p, s in zip(dev_para_sent_ids, dev_para_y_pred):
      label_str = "yes" if s == 1 else "no"
      f.write(f"{p}, {label_str} \n")
  print(f"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.para_dev_out}")

  # ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ì €ì¥ (test)
  with open(args.para_test_out, "w+") as f:
    f.write(f"id \t Predicted_Is_Paraphrase \n")
    for p, s in zip(test_para_sent_ids, test_para_y_pred):
      label_str = "yes" if s == 1 else "no"
      f.write(f"{p}, {label_str} \n")
  print(f"ğŸ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {args.para_test_out}")
  print(f"âœ… [{args.filepath}]ì— ê¸°ë°˜í•œ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!")



# âœ… ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì •ì˜ í•¨ìˆ˜
def get_args():

  parser = argparse.ArgumentParser()

  # íŒŒì¼ ê²½ë¡œ ê´€ë ¨ ì¸ì
  parser.add_argument("--para_train", type=str, default="data/quora-train.csv")
  parser.add_argument("--para_dev", type=str, default="data/quora-dev.csv")
  parser.add_argument("--para_test", type=str, default="data/quora-test-student.csv")
  parser.add_argument("--para_dev_out", type=str, default="predictions/para-dev-output.csv")
  parser.add_argument("--para_test_out", type=str, default="predictions/para-test-output.csv")

  # í•™ìŠµ ê´€ë ¨ ì„¤ì • ì¸ì
  parser.add_argument("--seed", type=int, default=11711)
  parser.add_argument("--epochs", type=int, default=5)
  parser.add_argument("--use_gpu", action='store_true')
  parser.add_argument("--batch_size", help='sst: 64, cfimdb: 8 can fit a 12GB GPU', type=int, default=64)
  parser.add_argument("--lr", type=float, help="learning rate", default=1e-5)
  parser.add_argument("--model_size", type=str,
                      help="The model size as specified on hugging face. DO NOT use the xl model.",
                      choices=['gpt2', 'gpt2-medium', 'gpt2-large'], default='gpt2')
  parser.add_argument("--soft_pos_ratio", type=float, default=0.1,
                    help="ë¹„êµì  ì •ë‹µì„ ë§ì¶˜ low-confidence ìƒ˜í”Œì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ (default=0.1)")

  args = parser.parse_args()
  return args



# ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ê´€ë ¨ ì¸ì ìë™ ì„¤ì • í•¨ìˆ˜
def add_arguments(args):
  
  if args.model_size == 'gpt2':
    args.d = 768
    args.l = 12
    args.num_heads = 12
  elif args.model_size == 'gpt2-medium':
    args.d = 1024
    args.l = 24
    args.num_heads = 16
  elif args.model_size == 'gpt2-large':
    args.d = 1280
    args.l = 36
    args.num_heads = 20
  else:
    raise Exception(f'{args.model_size} is not supported.')
  return args



# ì‹¤í–‰ ì‹œì‘ì : í•™ìŠµ ë° í‰ê°€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
if __name__ == "__main__":
  args = get_args()                                             # ì¸ì íŒŒì‹±
  args.filepath = f'{args.epochs}-{args.lr}-paraphrase.pt'      # ëª¨ë¸ ì €ì¥ íŒŒì¼ëª… êµ¬ì„±
  seed_everything(args.seed)                                    # ì¬í˜„ì„±ì„ ìœ„í•œ random seed ê³ ì •.

  train(args)   # ì „ì²´ í›ˆë ¨ + HNP ìˆ˜í–‰

  # HNP ì „ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
  args.para_dev_out = "predictions/para-dev-output-best5.csv"
  args.para_test_out = "predictions/para-test-output-best5.csv"
  test(args)

  # HNP í›„ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±
  args.filepath = args.filepath.replace(".pt", "-hnp.pt")
  args.para_dev_out = "predictions/para-dev-output-hnp6.csv"
  args.para_test_out = "predictions/para-test-output-hnp6.csv"
  test(args)