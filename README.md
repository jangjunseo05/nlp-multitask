# Multi-task NLP with GPT-2: Bridging the Gap between Model and Human Expectation

## ğŸ“Œ Project Overview
This project explores the boundaries of **Human-AI Interaction** by implementing a multi-task NLP system using GPT-2. The focus was on bridging the gap between model capacity and the inherent ambiguity of human-generated data across three tasks: Sentiment Classification, Paraphrase Detection, and Sonnet Generation.

## ğŸ› ï¸ Key Research Points & Decisions

### 1ï¸âƒ£ Sentiment Classification: Overcoming Imbalance
- **Strategy:** Implemented a **Coarse-to-Fine Cascade model** (2-stage & 3-stage) and data rephrasing techniques.
- **Research Decision:** After empirical evaluation, I opted for the **2-stage model** as the final architecture.My analysis showed that deeper cascades (3-stage) suffered from **error propagation**, proving that model complexity must be balanced with training stability in fine-grained tasks.

### 2ï¸âƒ£ Paraphrase Detection: Efficiency & Precision 
- **Strategy:** Applied **LoRA (Low-Rank Adaptation)** for parameter-efficient fine-tuning and **Hard Negative Priority (HNP)** training.
- **Research Decision:** Analyzed the **trade-off between confidence and noise** in HNP. While HNP targets difficult samples, I discovered that training on high-confidence incorrect samples can sometimes act as noise, leading to a refined strategy of merging hard negatives with original training data.

### 3ï¸âƒ£ Sonnet Generation: Aligning with Human Constraints 
- **Strategy:** Developed a Shakespearean sonnet generator incorporating a custom **Rhyme Loss** based on the cosine similarity of end-word embeddings.
- **Research Decision:** Implemented a 14-line post-processing logic to ensure structural fidelity. This resulted in a **significant improvement in Rhyme Accuracy (0.00 â†’ 0.12)** without degrading semantic coherence (BERTScore), demonstrating the efficacy of enforcing poetic constraints in generative models.

## ğŸ“Š Results & Honest Evaluation
* **Sentiment:** While rephrasing improved the F1-score (0.41 â†’ 0.44), the Cascade structure revealed a bottleneck in the coarse classifier's accuracy.
* **Paraphrase:** Baseline GPT-2 (5 epoch) remained more stable than HNP-only tuning, highlighting the sensitivity of confidence thresholds in sample selection.
* **Sonnet:** Successfully reduced Perplexity (35.68 â†’ 34.15) and improved structural completedness, though qualitative analysis still shows challenges in long-term semantic flow.

## ğŸ’¡ Human-centered Insights
This project reinforced my perspective that AI struggles not due to capacity, but due to the inherent ambiguity and noise in human-generated data.
  
## Guide
colabì—ì„œ https://github.com/hocheol0303/nlp/main.ipynb íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ í”„ë¡œì íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>
main.ipynb íŒŒì¼ì— ë“¤ì–´ê°€ì„œ ìœ„ë¶€í„° Shift + Enterë§Œ ì…ë ¥í•˜ë©´ Part-1ë¶€í„° Part-2ê¹Œì§€ ëª¨ë¸ì˜ í•™ìŠµ, ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥, test output csv ì €ì¥ì„ ì§„í–‰í•˜ë„ë¡ jupyter notebookì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

Part-2ì—ì„œ ì‚¬ìš©ë˜ëŠ” pronouncing bert_scoreë¥¼ installí•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤.(Part-1ì—ì„œëŠ” í™œìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)
í•´ë‹¹ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ ëª¨ë‘ ì‹¤í–‰ì‹œí‚¤ë©´ ê° í´ë”ì— ê° ëª¨ë¸ì˜ .ptíŒŒì¼(ëª¨ë¸ ê°€ì¤‘ì¹˜)ê³¼ test ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ íŒŒì¼ì´ ê° í´ë” ë‚´ì˜ predictions í´ë”ì— .csv íŒŒì¼ í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.

ì´ë¯¸ ì‹¤í–‰ë˜ì–´ìˆëŠ” ë‚´ìš©ì€ ë””ë²„ê¹…ì„ ìœ„í•´ ì„ì˜ì˜ ë°ì´í„°ì™€ ì„ì˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì—¬ ì‹¤í—˜í•œ ê²°ê³¼ì´ë‹ˆ, ë¶€ë””, ì±„ì ì— í•´ë‹¹ ë‚´ìš©ì´ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìœ¼ë©´ í•˜ëŠ” ë°”ëŒì…ë‹ˆë‹¤!

## Part-1
nlp2025-1_part1 í´ë”ì—ì„œ ì½”ë“œ ì±„ìš°ê¸° ê³¼ì œë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

## Part-2
nlp2025-1_part2 í´ë”ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê³¼ì œë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.
ë§Œë“¤ì–´ì§€ëŠ” ê²°ê³¼ë¬¼ì€ ë‹¤ìŒê³¼ ê°™ì´ ì´ 7ê°œì˜ ëª¨ë¸(classifier ëª¨ë¸ 4ê°œ, sonnet ëª¨ë¸ 1ê°œ, paraphrase ëª¨ë¸ 2ê°œ)ê³¼ 4ê°œì˜ test outputì´ ìˆìŠµë‹ˆë‹¤.
- Classifier
  - base ëª¨ë¸ì€ cascade ê¸°ë²•ì„ ì ìš©í•˜ì§€ ì•Šì€ ë‹¨ì¼ ëª¨ë¸ì…ë‹ˆë‹¤.
  - nlp2025-1_part2/sst-classifier_base.pt
  - nlp2025-1_part2/cfimdb-classifier_base.pt
  - nlp2025-1_part2/sst-classifier_cascade.pt
  - nlp2025-1_part2/cfimdb-classifier_cascade.pt
- Sonnet
  - nlp2025-1_part2/best_sonnet_generation.pt
- Paraphrase
  - Paraphrase ëª¨ë¸ì€ hard negative priorityì˜ ì ìš© ì—¬ë¶€ì— ë”°ë¼ ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤. 5epochë¥¼ í†µí•´ best ëª¨ë¸ì„ ì €ì¥ í›„ í•´ë‹¹ ëª¨ë¸ì„ loadí•˜ì—¬ hard negative priorityë¥¼ ì ìš©í•˜ê³  -hnp.pt ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
  - nlp2025-1_part2/{NUM}-1e-5-paraphrase.pt
  - nlp2025-1_part2/{NUM}-1e-5-paraphrase-hnp.pt

ëª¨ë¸ì˜ test outputì€ predictions í´ë”ì— ê°ê° ì €ì¥ë˜ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.

ì´ìƒìœ¼ë¡œ 9ì¡°ì˜ README.mdë¥¼ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤.
í•œ í•™ê¸°ë™ì•ˆ ì¢‹ì€ ê°•ì˜ë¥¼ í•´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤!
